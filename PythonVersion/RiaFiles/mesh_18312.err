
 Starting FDS ...

 MPI Process      0 started on tellico-compute1
 MPI Process      4 started on tellico-compute1
 MPI Process      6 started on tellico-compute1
 MPI Process      7 started on tellico-compute1
 MPI Process      1 started on tellico-compute1
 MPI Process      2 started on tellico-compute1
 MPI Process      3 started on tellico-compute1
 MPI Process      5 started on tellico-compute1

 Reading FDS input file ...

 Mesh 1 is assigned to MPI Process 0
 Mesh 2 is assigned to MPI Process 1
 Mesh 3 is assigned to MPI Process 2
 Mesh 4 is assigned to MPI Process 3
 Mesh 5 is assigned to MPI Process 4
 Mesh 6 is assigned to MPI Process 5
 Mesh 7 is assigned to MPI Process 6
 Mesh 8 is assigned to MPI Process 7
 Input file read
 OpenMP thread   0 of   3 assigned to MPI process      7 of      7
 OpenMP thread   1 of   3 assigned to MPI process      7 of      7
 OpenMP thread   0 of   3 assigned to MPI process      1 of      7
 OpenMP thread   2 of   3 assigned to MPI process      1 of      7
 OpenMP thread   0 of   3 assigned to MPI process      5 of      7
 OpenMP thread   3 of   3 assigned to MPI process      5 of      7
 OpenMP thread   2 of   3 assigned to MPI process      5 of      7
 OpenMP thread   0 of   3 assigned to MPI process      3 of      7
 OpenMP thread   1 of   3 assigned to MPI process      3 of      7
 OpenMP thread   0 of   3 assigned to MPI process      4 of      7
 OpenMP thread   3 of   3 assigned to MPI process      4 of      7
 OpenMP thread   1 of   3 assigned to MPI process      4 of      7
 OpenMP thread   2 of   3 assigned to MPI process      4 of      7
 OpenMP thread   3 of   3 assigned to MPI process      6 of      7
 OpenMP thread   0 of   3 assigned to MPI process      6 of      7
 OpenMP thread   1 of   3 assigned to MPI process      6 of      7
 OpenMP thread   2 of   3 assigned to MPI process      6 of      7
 OpenMP thread   3 of   3 assigned to MPI process      0 of      7
 OpenMP thread   0 of   3 assigned to MPI process      0 of      7
 OpenMP thread   2 of   3 assigned to MPI process      2 of      7
 OpenMP thread   0 of   3 assigned to MPI process      2 of      7
 OpenMP thread   3 of   3 assigned to MPI process      3 of      7
 OpenMP thread   3 of   3 assigned to MPI process      1 of      7
 OpenMP thread   1 of   3 assigned to MPI process      5 of      7
 OpenMP thread   1 of   3 assigned to MPI process      0 of      7
 OpenMP thread   2 of   3 assigned to MPI process      0 of      7
 OpenMP thread   1 of   3 assigned to MPI process      1 of      7
 OpenMP thread   1 of   3 assigned to MPI process      2 of      7
 OpenMP thread   3 of   3 assigned to MPI process      2 of      7
 Completed Initialization Step  1
 OpenMP thread   3 of   3 assigned to MPI process      7 of      7
 OpenMP thread   2 of   3 assigned to MPI process      7 of      7
 OpenMP thread   2 of   3 assigned to MPI process      3 of      7
 Completed Initialization Step  2
 Completed Initialization Step  3
 Completed Initialization Step  4
 Completed SETUP_PRESSURE_ZONES
 Completed INITIALIZE_MESH_EXCHANGE_2
 Completed INITIALIZE_MESH_VARIABLES_2
 Initialized Radiation

Program received signal SIGSEGV: Segmentation fault - invalid memory reference.

Backtrace for this error:

Program received signal SIGSEGV: Segmentation fault - invalid memory reference.

Backtrace for this error:
#0  0x2000000504d7 in ???
#1  0x2000009cb74c in ???
#2  0x200000612103 in buf_write
	at /tmp/leobardovalera/spack-stage/gcc-9.2.0-2ry5cb6xgiwxklssz64ywkderkrrcfbq/spack-src/libgfortran/io/unix.c:645
#3  0x200000612103 in buf_write
	at /tmp/leobardovalera/spack-stage/gcc-9.2.0-2ry5cb6xgiwxklssz64ywkderkrrcfbq/spack-src/libgfortran/io/unix.c:628
#4  0x200000608e2b in swrite
	at /tmp/leobardovalera/spack-stage/gcc-9.2.0-2ry5cb6xgiwxklssz64ywkderkrrcfbq/spack-src/libgfortran/io/unix.h:59
#5  0x200000608e2b in write_buf
	at /tmp/leobardovalera/spack-stage/gcc-9.2.0-2ry5cb6xgiwxklssz64ywkderkrrcfbq/spack-src/libgfortran/io/transfer.c:951
#6  0x200000609097 in unformatted_write
	at /tmp/leobardovalera/spack-stage/gcc-9.2.0-2ry5cb6xgiwxklssz64ywkderkrrcfbq/spack-src/libgfortran/io/transfer.c:1198
#7  0x2000006095cf in wrap_scalar_transfer
	at /tmp/leobardovalera/spack-stage/gcc-9.2.0-2ry5cb6xgiwxklssz64ywkderkrrcfbq/spack-src/libgfortran/io/transfer.c:2369
#8  0x2000006095cf in wrap_scalar_transfer
	at /tmp/leobardovalera/spack-stage/gcc-9.2.0-2ry5cb6xgiwxklssz64ywkderkrrcfbq/spack-src/libgfortran/io/transfer.c:2346
#9  0x1031fbdf in ???
#10  0x104ce8e7 in ???
#11  0x10003903 in ???
#12  0x2000009451ff in ???
#0  0x2000000504d7 in ???
#1  0x2000009cb74c in ???
#2  0x200000612103 in buf_write
	at /tmp/leobardovalera/spack-stage/gcc-9.2.0-2ry5cb6xgiwxklssz64ywkderkrrcfbq/spack-src/libgfortran/io/unix.c:645
#3  0x200000612103 in buf_write
	at /tmp/leobardovalera/spack-stage/gcc-9.2.0-2ry5cb6xgiwxklssz64ywkderkrrcfbq/spack-src/libgfortran/io/unix.c:628
#4  0x200000608e2b in swrite
	at /tmp/leobardovalera/spack-stage/gcc-9.2.0-2ry5cb6xgiwxklssz64ywkderkrrcfbq/spack-src/libgfortran/io/unix.h:59
#5  0x200000608e2b in write_buf
	at /tmp/leobardovalera/spack-stage/gcc-9.2.0-2ry5cb6xgiwxklssz64ywkderkrrcfbq/spack-src/libgfortran/io/transfer.c:951
#6  0x200000609097 in unformatted_write
	at /tmp/leobardovalera/spack-stage/gcc-9.2.0-2ry5cb6xgiwxklssz64ywkderkrrcfbq/spack-src/libgfortran/io/transfer.c:1198
#7  0x2000006095cf in wrap_scalar_transfer
	at /tmp/leobardovalera/spack-stage/gcc-9.2.0-2ry5cb6xgiwxklssz64ywkderkrrcfbq/spack-src/libgfortran/io/transfer.c:2369
#8  0x2000006095cf in wrap_scalar_transfer
	at /tmp/leobardovalera/spack-stage/gcc-9.2.0-2ry5cb6xgiwxklssz64ywkderkrrcfbq/spack-src/libgfortran/io/transfer.c:2346
#9  0x1031fbdf in ???
#10  0x104ce8e7 in ???
#11  0x10003903 in ???
#12  0x2000009451ff in ???
--------------------------------------------------------------------------
mpiexec noticed that process rank 6 with PID 0 on node tellico-compute1 exited on signal 11 (Segmentation fault).
--------------------------------------------------------------------------
